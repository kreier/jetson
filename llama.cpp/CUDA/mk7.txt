mk@jetson:~/llama.cpp3$ ./build/bin/main -m ../.cache/llama.cpp/TheBloke_TinyLlama-1.1B-Chat-v1.0-GGUF_tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf -p "Solar System" --n-gpu-layers 24 --ctx-size 512 --threads 4 --temp 0.7 --top-k 40 --top-p 0.9 --batch-size 16
Log start
main: build = 1618 (81bc9214)
main: built with gcc (GCC) 8.5.0 for aarch64-unknown-linux-gnu
main: seed  = 1743261459
ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no
ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes
ggml_init_cublas: found 1 CUDA devices:
  Device 0: NVIDIA Tegra X1, compute capability 5.3
llama_model_loader: loaded meta data with 23 key-value pairs and 201 tensors from ../.cache/llama.cpp/TheBloke_TinyLlama-1.1B-Chat-v1.0-GGUF_tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf (version GGUF V3 (latest))
llama_model_loader: - tensor    0:                    output.weight q6_K     [  2048, 32000,     1,     1 ]
llama_model_loader: - tensor    1:                token_embd.weight q4_K     [  2048, 32000,     1,     1 ]
llama_model_loader: - tensor    2:           blk.0.attn_norm.weight f32      [  2048,     1,     1,     1 ]
llama_model_loader: - tensor    3:            blk.0.ffn_down.weight q6_K     [  5632,  2048,     1,     1 ]
llama_model_loader: - tensor    4:            blk.0.ffn_gate.weight q4_K     [  2048,  5632,     1,     1 ]
llama_model_loader: - tensor    5:              blk.0.ffn_up.weight q4_K     [  2048,  5632,     1,     1 ]
llama_model_loader: - tensor    6:            blk.0.ffn_norm.weight f32      [  2048,     1,     1,     1 ]
llama_model_loader: - tensor    7:              blk.0.attn_k.weight q4_K     [  2048,   256,     1,     1 ]
llama_model_loader: - tensor    8:         blk.0.attn_output.weight q4_K     [  2048,  2048,     1,     1 ]
llama_model_loader: - tensor    9:              blk.0.attn_q.weight q4_K     [  2048,  2048,     1,     1 ]
llama_model_loader: - tensor   10:              blk.0.attn_v.weight q6_K     [  2048,   256,     1,     1 ]
llama_model_loader: - tensor   11:           blk.1.attn_norm.weight f32      [  2048,     1,     1,     1 ]
llama_model_loader: - tensor   12:            blk.1.ffn_down.weight q6_K     [  5632,  2048,     1,     1 ]
llama_model_loader: - tensor   13:            blk.1.ffn_gate.weight q4_K     [  2048,  5632,     1,     1 ]
llama_model_loader: - tensor   14:              blk.1.ffn_up.weight q4_K     [  2048,  5632,     1,     1 ]
llama_model_loader: - tensor   15:            blk.1.ffn_norm.weight f32      [  2048,     1,     1,     1 ]
llama_model_loader: - tensor   16:              blk.1.attn_k.weight q4_K     [  2048,   256,     1,     1 ]
llama_model_loader: - tensor   17:         blk.1.attn_output.weight q4_K     [  2048,  2048,     1,     1 ]
llama_model_loader: - tensor   18:              blk.1.attn_q.weight q4_K     [  2048,  2048,     1,     1 ]
llama_model_loader: - tensor   19:              blk.1.attn_v.weight q6_K     [  2048,   256,     1,     1 ]
llama_model_loader: - tensor   20:          blk.10.attn_norm.weight f32      [  2048,     1,     1,     1 ]
llama_model_loader: - tensor   21:           blk.10.ffn_down.weight q4_K     [  5632,  2048,     1,     1 ]
llama_model_loader: - tensor   22:           blk.10.ffn_gate.weight q4_K     [  2048,  5632,     1,     1 ]
llama_model_loader: - tensor   23:             blk.10.ffn_up.weight q4_K     [  2048,  5632,     1,     1 ]
llama_model_loader: - tensor   24:           blk.10.ffn_norm.weight f32      [  2048,     1,     1,     1 ]
llama_model_loader: - tensor   25:             blk.10.attn_k.weight q4_K     [  2048,   256,     1,     1 ]
llama_model_loader: - tensor   26:        blk.10.attn_output.weight q4_K     [  2048,  2048,     1,     1 ]
llama_model_loader: - tensor   27:             blk.10.attn_q.weight q4_K     [  2048,  2048,     1,     1 ]
llama_model_loader: - tensor   28:             blk.10.attn_v.weight q4_K     [  2048,   256,     1,     1 ]
llama_model_loader: - tensor   29:          blk.11.attn_norm.weight f32      [  2048,     1,     1,     1 ]
llama_model_loader: - tensor   30:           blk.11.ffn_down.weight q4_K     [  5632,  2048,     1,     1 ]
llama_model_loader: - tensor   31:           blk.11.ffn_gate.weight q4_K     [  2048,  5632,     1,     1 ]
llama_model_loader: - tensor   32:             blk.11.ffn_up.weight q4_K     [  2048,  5632,     1,     1 ]
llama_model_loader: - tensor   33:           blk.11.ffn_norm.weight f32      [  2048,     1,     1,     1 ]
llama_model_loader: - tensor   34:             blk.11.attn_k.weight q4_K     [  2048,   256,     1,     1 ]
llama_model_loader: - tensor   35:        blk.11.attn_output.weight q4_K     [  2048,  2048,     1,     1 ]
llama_model_loader: - tensor   36:             blk.11.attn_q.weight q4_K     [  2048,  2048,     1,     1 ]
llama_model_loader: - tensor   37:             blk.11.attn_v.weight q4_K     [  2048,   256,     1,     1 ]
llama_model_loader: - tensor   38:          blk.12.attn_norm.weight f32      [  2048,     1,     1,     1 ]
llama_model_loader: - tensor   39:           blk.12.ffn_down.weight q6_K     [  5632,  2048,     1,     1 ]
llama_model_loader: - tensor   40:           blk.12.ffn_gate.weight q4_K     [  2048,  5632,     1,     1 ]
llama_model_loader: - tensor   41:             blk.12.ffn_up.weight q4_K     [  2048,  5632,     1,     1 ]
llama_model_loader: - tensor   42:           blk.12.ffn_norm.weight f32      [  2048,     1,     1,     1 ]
llama_model_loader: - tensor   43:             blk.12.attn_k.weight q4_K     [  2048,   256,     1,     1 ]
llama_model_loader: - tensor   44:        blk.12.attn_output.weight q4_K     [  2048,  2048,     1,     1 ]
llama_model_loader: - tensor   45:             blk.12.attn_q.weight q4_K     [  2048,  2048,     1,     1 ]
llama_model_loader: - tensor   46:             blk.12.attn_v.weight q6_K     [  2048,   256,     1,     1 ]
llama_model_loader: - tensor   47:          blk.13.attn_norm.weight f32      [  2048,     1,     1,     1 ]
llama_model_loader: - tensor   48:           blk.13.ffn_down.weight q4_K     [  5632,  2048,     1,     1 ]
llama_model_loader: - tensor   49:           blk.13.ffn_gate.weight q4_K     [  2048,  5632,     1,     1 ]
llama_model_loader: - tensor   50:             blk.13.ffn_up.weight q4_K     [  2048,  5632,     1,     1 ]
llama_model_loader: - tensor   51:           blk.13.ffn_norm.weight f32      [  2048,     1,     1,     1 ]
llama_model_loader: - tensor   52:             blk.13.attn_k.weight q4_K     [  2048,   256,     1,     1 ]
llama_model_loader: - tensor   53:        blk.13.attn_output.weight q4_K     [  2048,  2048,     1,     1 ]
llama_model_loader: - tensor   54:             blk.13.attn_q.weight q4_K     [  2048,  2048,     1,     1 ]
llama_model_loader: - tensor   55:             blk.13.attn_v.weight q4_K     [  2048,   256,     1,     1 ]
llama_model_loader: - tensor   56:          blk.14.attn_norm.weight f32      [  2048,     1,     1,     1 ]
llama_model_loader: - tensor   57:           blk.14.ffn_down.weight q4_K     [  5632,  2048,     1,     1 ]
llama_model_loader: - tensor   58:           blk.14.ffn_gate.weight q4_K     [  2048,  5632,     1,     1 ]
llama_model_loader: - tensor   59:             blk.14.ffn_up.weight q4_K     [  2048,  5632,     1,     1 ]
llama_model_loader: - tensor   60:           blk.14.ffn_norm.weight f32      [  2048,     1,     1,     1 ]
llama_model_loader: - tensor   61:             blk.14.attn_k.weight q4_K     [  2048,   256,     1,     1 ]
llama_model_loader: - tensor   62:        blk.14.attn_output.weight q4_K     [  2048,  2048,     1,     1 ]
llama_model_loader: - tensor   63:             blk.14.attn_q.weight q4_K     [  2048,  2048,     1,     1 ]
llama_model_loader: - tensor   64:             blk.14.attn_v.weight q4_K     [  2048,   256,     1,     1 ]
llama_model_loader: - tensor   65:          blk.15.attn_norm.weight f32      [  2048,     1,     1,     1 ]
llama_model_loader: - tensor   66:           blk.15.ffn_down.weight q6_K     [  5632,  2048,     1,     1 ]
llama_model_loader: - tensor   67:           blk.15.ffn_gate.weight q4_K     [  2048,  5632,     1,     1 ]
llama_model_loader: - tensor   68:             blk.15.ffn_up.weight q4_K     [  2048,  5632,     1,     1 ]
llama_model_loader: - tensor   69:           blk.15.ffn_norm.weight f32      [  2048,     1,     1,     1 ]
llama_model_loader: - tensor   70:             blk.15.attn_k.weight q4_K     [  2048,   256,     1,     1 ]
llama_model_loader: - tensor   71:        blk.15.attn_output.weight q4_K     [  2048,  2048,     1,     1 ]
llama_model_loader: - tensor   72:             blk.15.attn_q.weight q4_K     [  2048,  2048,     1,     1 ]
llama_model_loader: - tensor   73:             blk.15.attn_v.weight q6_K     [  2048,   256,     1,     1 ]
llama_model_loader: - tensor   74:          blk.16.attn_norm.weight f32      [  2048,     1,     1,     1 ]
llama_model_loader: - tensor   75:           blk.16.ffn_down.weight q4_K     [  5632,  2048,     1,     1 ]
llama_model_loader: - tensor   76:           blk.16.ffn_gate.weight q4_K     [  2048,  5632,     1,     1 ]
llama_model_loader: - tensor   77:             blk.16.ffn_up.weight q4_K     [  2048,  5632,     1,     1 ]
llama_model_loader: - tensor   78:           blk.16.ffn_norm.weight f32      [  2048,     1,     1,     1 ]
llama_model_loader: - tensor   79:             blk.16.attn_k.weight q4_K     [  2048,   256,     1,     1 ]
llama_model_loader: - tensor   80:        blk.16.attn_output.weight q4_K     [  2048,  2048,     1,     1 ]
llama_model_loader: - tensor   81:             blk.16.attn_q.weight q4_K     [  2048,  2048,     1,     1 ]
llama_model_loader: - tensor   82:             blk.16.attn_v.weight q4_K     [  2048,   256,     1,     1 ]
llama_model_loader: - tensor   83:          blk.17.attn_norm.weight f32      [  2048,     1,     1,     1 ]
llama_model_loader: - tensor   84:           blk.17.ffn_down.weight q4_K     [  5632,  2048,     1,     1 ]
llama_model_loader: - tensor   85:           blk.17.ffn_gate.weight q4_K     [  2048,  5632,     1,     1 ]
llama_model_loader: - tensor   86:             blk.17.ffn_up.weight q4_K     [  2048,  5632,     1,     1 ]
llama_model_loader: - tensor   87:           blk.17.ffn_norm.weight f32      [  2048,     1,     1,     1 ]
llama_model_loader: - tensor   88:             blk.17.attn_k.weight q4_K     [  2048,   256,     1,     1 ]
llama_model_loader: - tensor   89:        blk.17.attn_output.weight q4_K     [  2048,  2048,     1,     1 ]
llama_model_loader: - tensor   90:             blk.17.attn_q.weight q4_K     [  2048,  2048,     1,     1 ]
llama_model_loader: - tensor   91:             blk.17.attn_v.weight q4_K     [  2048,   256,     1,     1 ]
llama_model_loader: - tensor   92:          blk.18.attn_norm.weight f32      [  2048,     1,     1,     1 ]
llama_model_loader: - tensor   93:           blk.18.ffn_down.weight q6_K     [  5632,  2048,     1,     1 ]
llama_model_loader: - tensor   94:           blk.18.ffn_gate.weight q4_K     [  2048,  5632,     1,     1 ]
llama_model_loader: - tensor   95:             blk.18.ffn_up.weight q4_K     [  2048,  5632,     1,     1 ]
llama_model_loader: - tensor   96:           blk.18.ffn_norm.weight f32      [  2048,     1,     1,     1 ]
llama_model_loader: - tensor   97:             blk.18.attn_k.weight q4_K     [  2048,   256,     1,     1 ]
llama_model_loader: - tensor   98:        blk.18.attn_output.weight q4_K     [  2048,  2048,     1,     1 ]
llama_model_loader: - tensor   99:             blk.18.attn_q.weight q4_K     [  2048,  2048,     1,     1 ]
llama_model_loader: - tensor  100:             blk.18.attn_v.weight q6_K     [  2048,   256,     1,     1 ]
llama_model_loader: - tensor  101:          blk.19.attn_norm.weight f32      [  2048,     1,     1,     1 ]
llama_model_loader: - tensor  102:           blk.19.ffn_down.weight q4_K     [  5632,  2048,     1,     1 ]
llama_model_loader: - tensor  103:           blk.19.ffn_gate.weight q4_K     [  2048,  5632,     1,     1 ]
llama_model_loader: - tensor  104:             blk.19.ffn_up.weight q4_K     [  2048,  5632,     1,     1 ]
llama_model_loader: - tensor  105:           blk.19.ffn_norm.weight f32      [  2048,     1,     1,     1 ]
llama_model_loader: - tensor  106:             blk.19.attn_k.weight q4_K     [  2048,   256,     1,     1 ]
llama_model_loader: - tensor  107:        blk.19.attn_output.weight q4_K     [  2048,  2048,     1,     1 ]
llama_model_loader: - tensor  108:             blk.19.attn_q.weight q4_K     [  2048,  2048,     1,     1 ]
llama_model_loader: - tensor  109:             blk.19.attn_v.weight q4_K     [  2048,   256,     1,     1 ]
llama_model_loader: - tensor  110:           blk.2.attn_norm.weight f32      [  2048,     1,     1,     1 ]
llama_model_loader: - tensor  111:            blk.2.ffn_down.weight q4_K     [  5632,  2048,     1,     1 ]
llama_model_loader: - tensor  112:            blk.2.ffn_gate.weight q4_K     [  2048,  5632,     1,     1 ]
llama_model_loader: - tensor  113:              blk.2.ffn_up.weight q4_K     [  2048,  5632,     1,     1 ]
llama_model_loader: - tensor  114:            blk.2.ffn_norm.weight f32      [  2048,     1,     1,     1 ]
llama_model_loader: - tensor  115:              blk.2.attn_k.weight q4_K     [  2048,   256,     1,     1 ]
llama_model_loader: - tensor  116:         blk.2.attn_output.weight q4_K     [  2048,  2048,     1,     1 ]
llama_model_loader: - tensor  117:              blk.2.attn_q.weight q4_K     [  2048,  2048,     1,     1 ]
llama_model_loader: - tensor  118:              blk.2.attn_v.weight q4_K     [  2048,   256,     1,     1 ]
llama_model_loader: - tensor  119:          blk.20.attn_norm.weight f32      [  2048,     1,     1,     1 ]
llama_model_loader: - tensor  120:           blk.20.ffn_down.weight q6_K     [  5632,  2048,     1,     1 ]
llama_model_loader: - tensor  121:           blk.20.ffn_gate.weight q4_K     [  2048,  5632,     1,     1 ]
llama_model_loader: - tensor  122:             blk.20.ffn_up.weight q4_K     [  2048,  5632,     1,     1 ]
llama_model_loader: - tensor  123:           blk.20.ffn_norm.weight f32      [  2048,     1,     1,     1 ]
llama_model_loader: - tensor  124:             blk.20.attn_k.weight q4_K     [  2048,   256,     1,     1 ]
llama_model_loader: - tensor  125:        blk.20.attn_output.weight q4_K     [  2048,  2048,     1,     1 ]
llama_model_loader: - tensor  126:             blk.20.attn_q.weight q4_K     [  2048,  2048,     1,     1 ]
llama_model_loader: - tensor  127:             blk.20.attn_v.weight q6_K     [  2048,   256,     1,     1 ]
llama_model_loader: - tensor  128:          blk.21.attn_norm.weight f32      [  2048,     1,     1,     1 ]
llama_model_loader: - tensor  129:           blk.21.ffn_down.weight q4_K     [  5632,  2048,     1,     1 ]
llama_model_loader: - tensor  130:           blk.21.ffn_gate.weight q4_K     [  2048,  5632,     1,     1 ]
llama_model_loader: - tensor  131:             blk.21.ffn_up.weight q4_K     [  2048,  5632,     1,     1 ]
llama_model_loader: - tensor  132:           blk.21.ffn_norm.weight f32      [  2048,     1,     1,     1 ]
llama_model_loader: - tensor  133:             blk.21.attn_k.weight q4_K     [  2048,   256,     1,     1 ]
llama_model_loader: - tensor  134:        blk.21.attn_output.weight q4_K     [  2048,  2048,     1,     1 ]
llama_model_loader: - tensor  135:             blk.21.attn_q.weight q4_K     [  2048,  2048,     1,     1 ]
llama_model_loader: - tensor  136:             blk.21.attn_v.weight q4_K     [  2048,   256,     1,     1 ]
llama_model_loader: - tensor  137:           blk.3.attn_norm.weight f32      [  2048,     1,     1,     1 ]
llama_model_loader: - tensor  138:            blk.3.ffn_down.weight q4_K     [  5632,  2048,     1,     1 ]
llama_model_loader: - tensor  139:            blk.3.ffn_gate.weight q4_K     [  2048,  5632,     1,     1 ]
llama_model_loader: - tensor  140:              blk.3.ffn_up.weight q4_K     [  2048,  5632,     1,     1 ]
llama_model_loader: - tensor  141:            blk.3.ffn_norm.weight f32      [  2048,     1,     1,     1 ]
llama_model_loader: - tensor  142:              blk.3.attn_k.weight q4_K     [  2048,   256,     1,     1 ]
llama_model_loader: - tensor  143:         blk.3.attn_output.weight q4_K     [  2048,  2048,     1,     1 ]
llama_model_loader: - tensor  144:              blk.3.attn_q.weight q4_K     [  2048,  2048,     1,     1 ]
llama_model_loader: - tensor  145:              blk.3.attn_v.weight q4_K     [  2048,   256,     1,     1 ]
llama_model_loader: - tensor  146:           blk.4.attn_norm.weight f32      [  2048,     1,     1,     1 ]
llama_model_loader: - tensor  147:            blk.4.ffn_down.weight q6_K     [  5632,  2048,     1,     1 ]
llama_model_loader: - tensor  148:            blk.4.ffn_gate.weight q4_K     [  2048,  5632,     1,     1 ]
llama_model_loader: - tensor  149:              blk.4.ffn_up.weight q4_K     [  2048,  5632,     1,     1 ]
llama_model_loader: - tensor  150:            blk.4.ffn_norm.weight f32      [  2048,     1,     1,     1 ]
llama_model_loader: - tensor  151:              blk.4.attn_k.weight q4_K     [  2048,   256,     1,     1 ]
llama_model_loader: - tensor  152:         blk.4.attn_output.weight q4_K     [  2048,  2048,     1,     1 ]
llama_model_loader: - tensor  153:              blk.4.attn_q.weight q4_K     [  2048,  2048,     1,     1 ]
llama_model_loader: - tensor  154:              blk.4.attn_v.weight q6_K     [  2048,   256,     1,     1 ]
llama_model_loader: - tensor  155:           blk.5.attn_norm.weight f32      [  2048,     1,     1,     1 ]
llama_model_loader: - tensor  156:            blk.5.ffn_down.weight q4_K     [  5632,  2048,     1,     1 ]
llama_model_loader: - tensor  157:            blk.5.ffn_gate.weight q4_K     [  2048,  5632,     1,     1 ]
llama_model_loader: - tensor  158:              blk.5.ffn_up.weight q4_K     [  2048,  5632,     1,     1 ]
llama_model_loader: - tensor  159:            blk.5.ffn_norm.weight f32      [  2048,     1,     1,     1 ]
llama_model_loader: - tensor  160:              blk.5.attn_k.weight q4_K     [  2048,   256,     1,     1 ]
llama_model_loader: - tensor  161:         blk.5.attn_output.weight q4_K     [  2048,  2048,     1,     1 ]
llama_model_loader: - tensor  162:              blk.5.attn_q.weight q4_K     [  2048,  2048,     1,     1 ]
llama_model_loader: - tensor  163:              blk.5.attn_v.weight q4_K     [  2048,   256,     1,     1 ]
llama_model_loader: - tensor  164:           blk.6.attn_norm.weight f32      [  2048,     1,     1,     1 ]
llama_model_loader: - tensor  165:            blk.6.ffn_down.weight q4_K     [  5632,  2048,     1,     1 ]
llama_model_loader: - tensor  166:            blk.6.ffn_gate.weight q4_K     [  2048,  5632,     1,     1 ]
llama_model_loader: - tensor  167:              blk.6.ffn_up.weight q4_K     [  2048,  5632,     1,     1 ]
llama_model_loader: - tensor  168:            blk.6.ffn_norm.weight f32      [  2048,     1,     1,     1 ]
llama_model_loader: - tensor  169:              blk.6.attn_k.weight q4_K     [  2048,   256,     1,     1 ]
llama_model_loader: - tensor  170:         blk.6.attn_output.weight q4_K     [  2048,  2048,     1,     1 ]
llama_model_loader: - tensor  171:              blk.6.attn_q.weight q4_K     [  2048,  2048,     1,     1 ]
llama_model_loader: - tensor  172:              blk.6.attn_v.weight q4_K     [  2048,   256,     1,     1 ]
llama_model_loader: - tensor  173:           blk.7.attn_norm.weight f32      [  2048,     1,     1,     1 ]
llama_model_loader: - tensor  174:            blk.7.ffn_down.weight q6_K     [  5632,  2048,     1,     1 ]
llama_model_loader: - tensor  175:            blk.7.ffn_gate.weight q4_K     [  2048,  5632,     1,     1 ]
llama_model_loader: - tensor  176:              blk.7.ffn_up.weight q4_K     [  2048,  5632,     1,     1 ]
llama_model_loader: - tensor  177:            blk.7.ffn_norm.weight f32      [  2048,     1,     1,     1 ]
llama_model_loader: - tensor  178:              blk.7.attn_k.weight q4_K     [  2048,   256,     1,     1 ]
llama_model_loader: - tensor  179:         blk.7.attn_output.weight q4_K     [  2048,  2048,     1,     1 ]
llama_model_loader: - tensor  180:              blk.7.attn_q.weight q4_K     [  2048,  2048,     1,     1 ]
llama_model_loader: - tensor  181:              blk.7.attn_v.weight q6_K     [  2048,   256,     1,     1 ]
llama_model_loader: - tensor  182:           blk.8.attn_norm.weight f32      [  2048,     1,     1,     1 ]
llama_model_loader: - tensor  183:            blk.8.ffn_down.weight q6_K     [  5632,  2048,     1,     1 ]
llama_model_loader: - tensor  184:            blk.8.ffn_gate.weight q4_K     [  2048,  5632,     1,     1 ]
llama_model_loader: - tensor  185:              blk.8.ffn_up.weight q4_K     [  2048,  5632,     1,     1 ]
llama_model_loader: - tensor  186:            blk.8.ffn_norm.weight f32      [  2048,     1,     1,     1 ]
llama_model_loader: - tensor  187:              blk.8.attn_k.weight q4_K     [  2048,   256,     1,     1 ]
llama_model_loader: - tensor  188:         blk.8.attn_output.weight q4_K     [  2048,  2048,     1,     1 ]
llama_model_loader: - tensor  189:              blk.8.attn_q.weight q4_K     [  2048,  2048,     1,     1 ]
llama_model_loader: - tensor  190:              blk.8.attn_v.weight q6_K     [  2048,   256,     1,     1 ]
llama_model_loader: - tensor  191:           blk.9.attn_norm.weight f32      [  2048,     1,     1,     1 ]
llama_model_loader: - tensor  192:            blk.9.ffn_down.weight q6_K     [  5632,  2048,     1,     1 ]
llama_model_loader: - tensor  193:            blk.9.ffn_gate.weight q4_K     [  2048,  5632,     1,     1 ]
llama_model_loader: - tensor  194:              blk.9.ffn_up.weight q4_K     [  2048,  5632,     1,     1 ]
llama_model_loader: - tensor  195:            blk.9.ffn_norm.weight f32      [  2048,     1,     1,     1 ]
llama_model_loader: - tensor  196:              blk.9.attn_k.weight q4_K     [  2048,   256,     1,     1 ]
llama_model_loader: - tensor  197:         blk.9.attn_output.weight q4_K     [  2048,  2048,     1,     1 ]
llama_model_loader: - tensor  198:              blk.9.attn_q.weight q4_K     [  2048,  2048,     1,     1 ]
llama_model_loader: - tensor  199:              blk.9.attn_v.weight q6_K     [  2048,   256,     1,     1 ]
llama_model_loader: - tensor  200:               output_norm.weight f32      [  2048,     1,     1,     1 ]
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = tinyllama_tinyllama-1.1b-chat-v1.0
llama_model_loader: - kv   2:                       llama.context_length u32              = 2048
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 2048
llama_model_loader: - kv   4:                          llama.block_count u32              = 22
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 5632
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 64
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 4
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0,000010
llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000,000000
llama_model_loader: - kv  11:                          general.file_type u32              = 15
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0,000000, 0,000000, 0,000000, 0,0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,61249]   = ["▁ t", "e r", "i n", "▁ a", "e n...
llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 2
llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {% for message in messages %}\n{% if m...
llama_model_loader: - kv  22:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   45 tensors
llama_model_loader: - type q4_K:  135 tensors
llama_model_loader: - type q6_K:   21 tensors
llm_load_vocab: special tokens definition check successful ( 259/32000 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32000
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 4
llm_load_print_meta: n_layer          = 22
llm_load_print_meta: n_rot            = 64
llm_load_print_meta: n_gqa            = 8
llm_load_print_meta: f_norm_eps       = 0,0e+00
llm_load_print_meta: f_norm_rms_eps   = 1,0e-05
llm_load_print_meta: f_clamp_kqv      = 0,0e+00
llm_load_print_meta: f_max_alibi_bias = 0,0e+00
llm_load_print_meta: n_ff             = 5632
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000,0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = ?B
llm_load_print_meta: model ftype      = mostly Q4_K - Medium
llm_load_print_meta: model params     = 1,10 B
llm_load_print_meta: model size       = 636,18 MiB (4,85 BPW)
llm_load_print_meta: general.name     = tinyllama_tinyllama-1.1b-chat-v1.0
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 2 '</s>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 2 '</s>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0,07 MiB
llm_load_tensors: using CUDA for GPU acceleration
llm_load_tensors: mem required  =   35,23 MiB
llm_load_tensors: offloading 22 repeating layers to GPU
llm_load_tensors: offloading non-repeating layers to GPU
llm_load_tensors: offloaded 24/25 layers to GPU
llm_load_tensors: VRAM used: 601,02 MiB
..................................................................................
llama_new_context_with_model: n_ctx      = 512
llama_new_context_with_model: freq_base  = 10000,0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init: offloading v cache to GPU
llama_kv_cache_init: VRAM kv self = 5,50 MiB
llama_new_context_with_model: kv self size  =   11,00 MiB
llama_build_graph: non-view tensors processed: 466/466
llama_new_context_with_model: compute buffer total size = 5,14 MiB
llama_new_context_with_model: VRAM scratch buffer: 2,08 MiB
llama_new_context_with_model: total VRAM used: 608,60 MiB (model: 601,02 MiB, context: 7,58 MiB)

system_info: n_threads = 4 / 4 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 |
sampling:
        repeat_last_n = 64, repeat_penalty = 1,100, frequency_penalty = 0,000, presence_penalty = 0,000
        top_k = 40, tfs_z = 1,000, top_p = 0,900, min_p = 0,050, typical_p = 1,000, temp = 0,700
        mirostat = 0, mirostat_lr = 0,100, mirostat_ent = 5,000
sampling order:
CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp
generate: n_ctx = 512, n_batch = 16, n_predict = -1, n_keep = 0


 Solar System
The Earth's position in the solar system is determined by its distance from the sun. The Earth orbits around the sun at a rate of 12,700 miles per hour (20,436 kilometers per hour). This means that the Earth completes one orbit around the sun every 365 days or approximately 24 hours and 29 minutes.

The solar system's distance from the sun varies throughout the year. In July, the Earth is closest to the sun; in December, it is farthest away. During these times, the Earth's orbit is tilted towards the sun, causing it to experience longer days and shorter nights.

The seasons are caused by the variations in the amount of radiation received from the sun during different parts of the year. The rate of solar radiation changes with time, and this variation affects the length of daylight and the timing of plant growth. This change in light levels is called seasonal change.

Seasons also affect the Earth's weather patterns. During spring (March to May), warmth causes vegetation growth, while in summer (June to August), vegetation begins to die off. These changes can lead to drought or floods depending on the region and climate.

The movement of the planets around the sun also affects the Earth's climate. The gravitational pull from the sun causes the planets to move in their orbits. This movement results in temperature variations, as each planet has a different rate of heat loss due to its unique properties.

For example, Mercury, which is the closest planet to the sun, experiences the greatest heat loss. Venus, which orbits further away, experiences the least heat loss. Mars, which is farthest from the sun, experiences the most extreme seasonal variation, with temperatures ranging from -173°F (-995°C) to -180°F (-982°C).

Overall, the Earth's position in the solar system and its movements around it are fundamental to our understanding of planetary motion and how planets interact with each other. [end of text]

llama_print_timings:        load time =    2951,76 ms
llama_print_timings:      sample time =     363,22 ms /   460 runs   (    0,79 ms per token,  1266,46 tokens per second)
llama_print_timings: prompt eval time =    1201,75 ms /     4 tokens (  300,44 ms per token,     3,33 tokens per second)
llama_print_timings:        eval time =  149901,40 ms /   459 runs   (  326,58 ms per token,     3,06 tokens per second)
llama_print_timings:       total time =  151735,32 ms
Log end
mk@jetson:~/llama.cpp3$